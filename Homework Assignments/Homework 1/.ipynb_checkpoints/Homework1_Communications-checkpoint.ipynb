{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a5512b7-8da5-4239-8c29-c863a5a09752",
   "metadata": {},
   "source": [
    "Jacqueline Sanchez <br> \n",
    "August 22, 2024 <br> \n",
    "Fall '24 | NLP <br>\n",
    "                                        Homework 1 | Communications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18723fd8-1f27-4071-998a-ba87205e3c18",
   "metadata": {},
   "source": [
    "Q1: Consider the following mini corpus containing 4 documents: [2 points] <br>\n",
    "```\n",
    "Doc 1: new home sales top forecasts\n",
    "Doc 2: home sales rise in july\n",
    "Doc 3: increase in home sales in july\n",
    "Doc 4: july new home sales rise\n",
    "\n",
    "```\n",
    "Compute the **inverted index** for this collection as discussed in class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a092c6-6e5c-4788-8c0e-34076db22e73",
   "metadata": {},
   "source": [
    "Creating a function that will create the inverted indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "763f3f56-7f47-47e2-9236-df2a9f270e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {\n",
    "    1: \"new home sales top forecasts\", \n",
    "    2: \"home sales rise in july\", \n",
    "    3: \"increase in home sales in july\", \n",
    "    4: \"july new home sales rise\"\n",
    "}\n",
    "\n",
    "def create_inverted_index(docs):\n",
    "    inverted_index = {}\n",
    "\n",
    "# Loop through each document\n",
    "    for doc_id, text in docs.items():\n",
    "        # Split the document text into words\n",
    "        words = text.split()\n",
    "\n",
    "# Loop through each word\n",
    "        for word in words:\n",
    "            # Add the word to the inverted if not present\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = []\n",
    "\n",
    "            # Add the document ID to the word's list if not already present \n",
    "            if doc_id not in inverted_index[word]:\n",
    "                inverted_index[word].append(doc_id)\n",
    "\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed12c1a-57df-4a44-b912-8eda89036be2",
   "metadata": {},
   "source": [
    "Using the 'inverted_index' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1615a712-5f4f-40bc-9dc7-65f1e6b64782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the inverted index\n",
    "inverted_index = create_inverted_index(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3051a7af-250b-4879-86ca-3b51863df55a",
   "metadata": {},
   "source": [
    "printing out the token's and id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7891801-9782-421d-8359-12d41b3c2304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new: [1, 4]\n",
      "home: [1, 2, 3, 4]\n",
      "sales: [1, 2, 3, 4]\n",
      "top: [1]\n",
      "forecasts: [1]\n",
      "rise: [2, 4]\n",
      "in: [2, 3]\n",
      "july: [2, 3, 4]\n",
      "increase: [3]\n"
     ]
    }
   ],
   "source": [
    "for token, doc_ids in inverted_index.items():\n",
    "    print(f\"{token}: {doc_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ae3dc-4c92-4732-a619-5f85068acec3",
   "metadata": {},
   "source": [
    "<span style=\"color: blue;\">\n",
    "\n",
    "**Notes:**<br>\n",
    "**Issue:** Full table scans slow down database performance during text searches. <br>\n",
    "\n",
    "**Goal:** Inverted indexes improve search efficiency by enabling faster document retrieval. <br>\n",
    "\n",
    "**How:** <br>\n",
    "Mapping: Indexes map each word to the documents where it appears.\n",
    "Retrieval: Search queries quickly locate documents using this mapping, avoiding full scans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071239c-7e33-4292-9086-a51059a9bac5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ade1f95-5810-4f14-8bac-b05a62fef28e",
   "metadata": {},
   "source": [
    "Q2: What is the time complexity (ğ‘‚ğ‘‚) in terms of the length of the posting lists for the most efficient way of computing/retrieving\n",
    "all documents for the following queries? [4 points] <br>\n",
    "\n",
    "(a) ğ‘ AND ğ‘ <br>\n",
    "(b) NOT ğ‘ <br>\n",
    "(c) NOT ğ‘ <br>\n",
    "(d) ğ‘ OR ğ‘ (assuming you have a hash set data-structure which can perform lookups in ğ‘‚(1), i.e., constant time) <br> <br>\n",
    "For terms/words, ğ‘, ğ‘, the length of the posting list is ğ¿(ğ‘), ğ¿(ğ‘). So your final answer should be in big-ğ‘‚ notation. Something like\n",
    "ğ‘‚(ğ‘”(â‹…)) where ğ‘”(â‹…) is a function of ğ¿(ğ‘), ğ¿(ğ‘). Also assume that the total number of documents in the corpus is ğ‘. Show/give\n",
    "reasons as to how you arrived at your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56c0b6-65ba-45b5-a73c-586cc1836699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77a337e4-4ac2-44cc-bb1c-46bd2560363f",
   "metadata": {},
   "source": [
    "Q3. Recommend the most efficient query processing order for the following query: [2 point] <br> <br>\n",
    "*kaleidoscope* **AND** *tangerine* **AND** *marmalade* **AND** *trees*\n",
    "Assume that these terms in a given corpus have the following length of their posting lists: <br>\n",
    "```\n",
    "Term          Size of posting list\n",
    "\n",
    "kaleidoscope    87009\n",
    "marmalade       107913\n",
    "tangerine       46653\n",
    "trees           316812\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ef2ad-e0ee-4215-92a0-af6faaab57e5",
   "metadata": {},
   "source": [
    "slide 37: https://www2.cs.uh.edu/~arjun/courses/nlp_ugrad/slides_linked/bool_txt_retrieval_Schtuetze.pdf<br>\n",
    "Simple and effective optimization: **Process in order of increasing frequency** <br> \n",
    "\n",
    "why? : To optimize query processing, it is most efficient to process search terms in order of increasing frequency of their posting lists. By starting with the least frequent term, which appears in fewer documents, you quickly narrow down the dataset. This reduces the number of documents for subsequent intersections, making the search process more efficient. As you add more terms, moving from smaller to larger posting lists, the dataset size remains manageable, minimizing computational load and aligning with principles such as Zipfâ€™s Law, which reflects the common distribution of term frequencies in a corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c0941-aa52-4859-b867-6b6185f3cba9",
   "metadata": {},
   "source": [
    "Q4: For what value of ğœŒ and ğµ does the Mandelbrotâ€™s law ğ‘“ = ğ‘ƒ(ğ‘Ÿ + ğœŒ)âˆ’ğµ becomes the Zipfâ€™s law? [2 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24906c6b-4b0f-4545-8637-9496d146f2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
